# ğŸ“Š Bank Marketing â€” Single Decision Tree Trainer

Entrenador de un Ãºnico Ã¡rbol de decisiÃ³n (sin ensembles) para el dataset clÃ¡sico de marketing bancario. Guarda el pipeline calibrado, registra mÃ©tricas en MongoDB y prioriza reproducibilidad.

---

## ğŸš€ Â¿QuÃ© incluye?

- Entrenamiento de un Ãºnico `DecisionTreeClassifier` con One-Hot Encoder de categorÃ­as infrecuentes.
- Tuning activado por defecto (bÃºsqueda aleatoria 80Ã—5) para mejorar el ranking de probabilidades.
- CalibraciÃ³n por defecto con `sigmoid` para probabilidades mÃ¡s Ãºtiles.
- UmbralizaciÃ³n opcional con piso de precisiÃ³n (Ãºtil en modo operativo).
- Registro de mÃ©tricas en **MongoDB** y guardado del artefacto en `artifacts/`.

---

## ğŸ§± Estructura del proyecto

```
bank-marketing-ml-mvc/
â”œâ”€â”€ artifacts/                 # Modelos entrenados (.joblib, con timestamp)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ bank-full.csv          # Dataset de entrenamiento (separado por ';')
â”œâ”€â”€ integrations/
â”‚   â”œâ”€â”€ featurize.py           # IngenierÃ­a de variables
â”‚   â””â”€â”€ mongo_repo.py          # Registro de mÃ©tricas en MongoDB
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py               # Entrenamiento principal (CLI)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ InstalaciÃ³n (Windows PowerShell)

1) Crear entorno y activarlo

```
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

2) Instalar dependencias

```
python -m pip install -r requirements.txt
```

3) Variables de entorno (MongoDB)

Crea un archivo `.env` en la raÃ­z del proyecto:

```
MONGO_URI=mongodb://localhost:27017
MONGO_DB=bank_marketing
```

---

## ğŸ§  Entrenamiento rÃ¡pido

Con los valores por defecto dejamos listo un modo enfocado a reproducibilidad y buen AP (~0.55 con un solo DT), sin pasar flags:

```
python scripts/train.py
```

Esto harÃ¡:
- Cargar `data/bank-full.csv` (separador `;`).
- Aplicar ingenierÃ­a de variables.
- Hacer tuning 80Ã—5 del Ã¡rbol.
- Calibrar probabilidades con `sigmoid`.
- Guardar el pipeline en `artifacts/DecisionTree_YYYYMMDD_HHMMSS.joblib`.
- Registrar mÃ©tricas en MongoDB.

MÃ©tricas esperadas con un Ãºnico DT (aprox.):
- average_precision â‰ˆ 0.54â€“0.56
- precision/recall dependerÃ¡n del umbral Ã³ptimo interno (optimize=f1 por defecto).

Nota: Por diseÃ±o, un solo Ã¡rbol no alcanza APâ‰¥0.60 de forma robusta en este dataset sin incurrir en leakage o ensembles.

---

## ï¿½ Modos de operaciÃ³n

### 1) Modo AP puro (por defecto)

- tuning activado (80Ã—5)
- calibraciÃ³n: `sigmoid`
- sin re-muestreo de clases
- optimize: `f1`

Ejecuta sin flags:

```
python scripts/train.py
```

### 2) Modo operativo con piso de precisiÃ³n (â‰¥ 0.62)

Maximiza el recall sujeto a una precisiÃ³n mÃ­nima, Ãºtil cuando los falsos positivos tienen mayor costo.

Ejemplo recomendado:

```
python scripts/train.py --optimize recall --min-precision 0.62 --resample over --resample-ratio 0.5
```

Observaciones tÃ­picas (aprox.):
- precision â‰ˆ 0.62â€“0.63
- recall â‰ˆ 0.35â€“0.40
- average_precision â‰ˆ 0.54â€“0.55

### 3) Evitar leakage por `duration`

`duration` sÃ³lo se conoce despuÃ©s de la llamada; para una evaluaciÃ³n realista:

```
python scripts/train.py --drop-duration
```

---

## ï¿½ ParÃ¡metros principales (CLI)

- `--optimize {f1,fbeta,precision,recall,cost}`: objetivo de umbralizaciÃ³n (por defecto: f1).
- `--min-precision FLOAT`: piso de precisiÃ³n (omitir por defecto).
- `--tune-dt` (activado por defecto): habilita tuning del Ã¡rbol.
- `--tune-iter INT` (defecto: 80) y `--tune-folds INT` (defecto: 5).
- `--calibration {sigmoid,isotonic,none}` (defecto: sigmoid).
- `--resample {none,over,under,smote}` (defecto: none) y `--resample-ratio FLOAT` (defecto: 0.5).
- `--drop-duration`: elimina columnas de duraciÃ³n (y derivadas) para evitar leakage.
- `--no-feat`: desactiva la ingenierÃ­a de variables interna.

---

## ğŸ§ª Reproducibilidad

- `random_state=42` por defecto y particiones estratificadas.
- El encoder agrupa categorÃ­as infrecuentes para reducir ruido en el ranking.
- CalibraciÃ³n separada en hold-out para mejorar las probabilidades.

---

## ğŸ“Œ Notas y lÃ­mites conocidos

- Requisito cumplido: sÃ³lo se usa un `DecisionTreeClassifier` (sin ensembles).
- En este dataset, AP â‰ˆ 0.55 es un techo razonable con un Ãºnico Ã¡rbol y sin leakage.
- Ensembles (RF/GBM/XGB) mejoran AP, pero no se usan para cumplir la restricciÃ³n del profesor.

---

## ğŸ—ºï¸ Siguientes pasos (opcionales)

- Exportar el Ã¡rbol a Graphviz para el informe:

```
# Ejemplo rÃ¡pido
from sklearn import tree
import joblib
model = joblib.load('artifacts/DecisionTree_YYYYMMDD_HHMMSS.joblib')
dt = model.named_steps['model'].base_estimator if hasattr(model, 'named_steps') else model
# Si calibrado, extraer el estimator subyacente
if hasattr(dt, 'calibrated_classifiers_'):
    dt = dt.calibrated_classifiers_[0].estimator
tree.export_graphviz(dt, out_file='tree.dot', filled=True, feature_names=None)
```

---

## ğŸ§° TecnologÃ­as

- scikit-learn, pandas, numpy, scipy, joblib
- imbalanced-learn
- pymongo, python-dotenv

